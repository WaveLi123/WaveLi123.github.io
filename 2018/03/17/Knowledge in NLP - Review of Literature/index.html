<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Knowledge in NLP - Review of Literature | 大道至简--博客记</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="/css/donate.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Knowledge in NLP - Review of Literature</h1><a id="logo" href="/.">大道至简--博客记</a><p class="description">道&amp;术</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Knowledge in NLP - Review of Literature</h1><div class="post-meta">Mar 17, 2018<span> | </span><span class="category"><a href="/categories/Paper-Share/">Paper Share</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2018/03/17/Knowledge in NLP - Review of Literature/" href="/2018/03/17/Knowledge in NLP - Review of Literature/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Knowledge-Outline"><span class="toc-number">1.</span> <span class="toc-text">Knowledge Outline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Leveraging-Knowledge-Bases-in-LSTMs-for-Improving-Machine-Reading-2017-ACL"><span class="toc-number">2.</span> <span class="toc-text">Leveraging Knowledge Bases in LSTMs for Improving Machine Reading, 2017 ACL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivation"><span class="toc-number">2.1.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-number">2.2.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-aware-Bidirectional-LSTMS"><span class="toc-number">2.2.1.</span> <span class="toc-text">Knowledge-aware Bidirectional LSTMS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-Knowledge-Base-Concepts"><span class="toc-number">2.2.2.</span> <span class="toc-text">Embedding Knowledge Base Concepts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments"><span class="toc-number">2.3.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Entity-Extraction"><span class="toc-number">2.3.1.</span> <span class="toc-text">Entity Extraction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Event-Extraction"><span class="toc-number">2.3.2.</span> <span class="toc-text">Event Extraction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#World-Knowledge-for-Reading-Comprehension-Rare-Entity-Prediction-with-Hierarchical-LSTMs-Using-External-Descriptions-2017-EMNLP"><span class="toc-number">3.</span> <span class="toc-text">World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions, 2017 EMNLP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Knowledge-Form-Data"><span class="toc-number">3.1.</span> <span class="toc-text">Knowledge Form (Data)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model"><span class="toc-number">3.2.</span> <span class="toc-text">Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-number">3.3.</span> <span class="toc-text">Results</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Knowledge-Grounded-Neural-Conversation-Model-2017-arXive"><span class="toc-number">4.</span> <span class="toc-text">A Knowledge-Grounded Neural Conversation Model, 2017 arXive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Knowledge-Form"><span class="toc-number">4.1.</span> <span class="toc-text">Knowledge Form</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Architecture"><span class="toc-number">4.2.</span> <span class="toc-text">Model Architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yuanfudao-at-SemEval-2018-Task-11-Three-way-Attention-and-Relational-Knowledge-for-Commonsense-Machine-Comprehension"><span class="toc-number">5.</span> <span class="toc-text">Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Knowledge"><span class="toc-number">5.1.</span> <span class="toc-text">Knowledge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Architecture-1"><span class="toc-number">5.2.</span> <span class="toc-text">Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Input-Layer"><span class="toc-number">5.2.1.</span> <span class="toc-text">Input Layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attention-Layer"><span class="toc-number">5.2.2.</span> <span class="toc-text">Attention Layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Output-Layer"><span class="toc-number">5.2.3.</span> <span class="toc-text">Output Layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments-1"><span class="toc-number">5.3.</span> <span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Thinking"><span class="toc-number">6.</span> <span class="toc-text">Thinking</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Feedback-amp-Advice"><span class="toc-number">7.</span> <span class="toc-text">Feedback&Advice</span></a></li></ol></div></div><div class="post-content"><hr>
<p>[TOC]</p>
<h2 id="Knowledge-Outline"><a href="#Knowledge-Outline" class="headerlink" title="Knowledge Outline"></a>Knowledge Outline</h2><ul>
<li>Type: Unstructured&#x3001;Structured</li>
<li>Methods: Theory &#x3001;Applying</li>
</ul>
<h2 id="Leveraging-Knowledge-Bases-in-LSTMs-for-Improving-Machine-Reading-2017-ACL"><a href="#Leveraging-Knowledge-Bases-in-LSTMs-for-Improving-Machine-Reading-2017-ACL" class="headerlink" title="Leveraging Knowledge Bases in LSTMs for Improving Machine Reading, 2017 ACL"></a>Leveraging Knowledge Bases in LSTMs for Improving Machine Reading, 2017 ACL</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><blockquote>
<p>How to add <strong>knowledge base</strong> to LSTM<br>Adaptive attention way</p>
</blockquote>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Knowledge-aware-Bidirectional-LSTMS"><a href="#Knowledge-aware-Bidirectional-LSTMS" class="headerlink" title="Knowledge-aware Bidirectional LSTMS"></a>Knowledge-aware Bidirectional LSTMS</h4><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.54.44.png" alt="Alt text"></p>
<p> <img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.56.10.png" alt="Alt text"></p>
<p> <img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.56.19.png" alt="Alt text"></p>
<p> <img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.56.25.png" alt="Alt text"></p>
<p> <img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.56.32.png" alt="Alt text"></p>
<p> <img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;2.56.40.png" alt="Alt text"></p>
<h4 id="Embedding-Knowledge-Base-Concepts"><a href="#Embedding-Knowledge-Base-Concepts" class="headerlink" title="Embedding Knowledge Base Concepts"></a>Embedding Knowledge Base Concepts</h4><ul>
<li>WordNet and NELL</li>
<li>(e1, r, e2)<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;3.06.57.png" alt="Alt text"></li>
</ul>
<p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;3.07.15.png" alt="Alt text"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Entity-Extraction"><a href="#Entity-Extraction" class="headerlink" title="Entity Extraction"></a>Entity Extraction</h4><ul>
<li>Data: ACE2005&#x3001;OntoNotes 5.0</li>
<li>Results:<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;3.22.38.png" alt="Alt text"><br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;3.22.50.png" alt="Alt text"></li>
</ul>
<h4 id="Event-Extraction"><a href="#Event-Extraction" class="headerlink" title="Event Extraction"></a>Event Extraction</h4><ul>
<li>Data: ACE2005</li>
<li>Results:<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-12 &#x4E0B;&#x5348;3.23.46.png" alt="Alt text"></li>
</ul>
<h2 id="World-Knowledge-for-Reading-Comprehension-Rare-Entity-Prediction-with-Hierarchical-LSTMs-Using-External-Descriptions-2017-EMNLP"><a href="#World-Knowledge-for-Reading-Comprehension-Rare-Entity-Prediction-with-Hierarchical-LSTMs-Using-External-Descriptions-2017-EMNLP" class="headerlink" title="World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions, 2017 EMNLP"></a>World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions, 2017 EMNLP</h2><h3 id="Knowledge-Form-Data"><a href="#Knowledge-Form-Data" class="headerlink" title="Knowledge Form (Data)"></a>Knowledge Form (Data)</h3><ul>
<li>Wikilinks Dataset -&gt; Wikilinks Rare Entity Prediction<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0A;&#x5348;11.13.52.png" alt="Alt text"></li>
</ul>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li><p>Double Encoder<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0A;&#x5348;11.14.29.png" alt="Alt text"></p>
</li>
<li><p>Hierarchical Double Encoder<br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0A;&#x5348;11.14.55.png" alt="Alt text"></p>
</li>
</ul>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0A;&#x5348;11.16.18.png" alt="Alt text"></p>
<h2 id="A-Knowledge-Grounded-Neural-Conversation-Model-2017-arXive"><a href="#A-Knowledge-Grounded-Neural-Conversation-Model-2017-arXive" class="headerlink" title="A Knowledge-Grounded Neural Conversation Model, 2017 arXive"></a>A Knowledge-Grounded Neural Conversation Model, 2017 arXive</h2><h3 id="Knowledge-Form"><a href="#Knowledge-Form" class="headerlink" title="Knowledge Form"></a>Knowledge Form</h3><ul>
<li>(Named Entities, Free-form Text)</li>
<li>Twitter, Foursquare</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0A;&#x5348;11.32.10.png" alt="Alt text"></p>
<h2 id="Yuanfudao-at-SemEval-2018-Task-11-Three-way-Attention-and-Relational-Knowledge-for-Commonsense-Machine-Comprehension"><a href="#Yuanfudao-at-SemEval-2018-Task-11-Three-way-Attention-and-Relational-Knowledge-for-Commonsense-Machine-Comprehension" class="headerlink" title="Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension"></a>Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension</h2><h3 id="Knowledge"><a href="#Knowledge" class="headerlink" title="Knowledge"></a>Knowledge</h3><ul>
<li>ConcepNet</li>
</ul>
<h3 id="Model-Architecture-1"><a href="#Model-Architecture-1" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0B;&#x5348;4.43.28.png" alt="Alt text"></p>
<h4 id="Input-Layer"><a href="#Input-Layer" class="headerlink" title="Input Layer"></a>Input Layer</h4><ul>
<li>Glove Embeddings, 300d</li>
<li>Part-of-speech and named-entity embeddings, 12d, 8d</li>
<li>Relation embeddings, 10d</li>
<li>Handcrafted Features, 2d</li>
</ul>
<h4 id="Attention-Layer"><a href="#Attention-Layer" class="headerlink" title="Attention Layer"></a>Attention Layer</h4><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0B;&#x5348;4.48.53.png" alt="Alt text"><br><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0B;&#x5348;4.49.22.png" alt="Alt text"></p>
<h4 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h4><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0B;&#x5348;4.50.02.png" alt="Alt text"></p>
<h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/2018/03/17/Knowledge in NLP - Review of Literature/&#x5C4F;&#x5E55;&#x5FEB;&#x7167; 2018-03-15 &#x4E0B;&#x5348;4.50.23.png" alt="Alt text"></p>
<h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><ul>
<li>Knowledge Embedding</li>
</ul>
<h2 id="Feedback-amp-Advice"><a href="#Feedback-amp-Advice" class="headerlink" title="Feedback&amp;Advice"></a>Feedback&amp;Advice</h2><ul>
<li>Weibo&#xFF1A;<a href="http://weibo.com/u/3165962554/home?wvr=5&amp;lf=reg" target="_blank" rel="noopener">@&#x4F1F;&#x5EB7;&#x9752;&#x5E74;</a>&#xFF0C;<a href="https://waveli123.github.io/" target="_blank" rel="noopener">@github</a></li>
<li>mail&#xFF1A;<a href="mailto:wavejkd@pku.edu.cn">wavejkd@pku.edu.cn</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2018/03/17/Knowledge in NLP - Review of Literature/" data-id="cjeurq4en000aii2bm8w3twms" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Paper-Share-20180312-ACL-2017-Carnegie-Mellon-University/">(Paper Share-20180312)[ACL 2017| Carnegie Mellon University]</a></div><div class="post-nav"><a href="/2018/03/12/WikiReading - Review of Literature/" class="next">WikiReading - Review of Literature</a></div><div data-thread-key="2018/03/17/Knowledge in NLP - Review of Literature/" data-title="Knowledge in NLP - Review of Literature" data-url="http://yoursite.com/2018/03/17/Knowledge in NLP - Review of Literature/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div class="post-donate"><div id="donate_board" class="donate_bar center"><a id="btn_donate" href="javascript:;" title="打赏" class="btn_donate"></a><div class="donate_txt"> &uarr;<br>此文有用? 求鼓励!<br></div></div><div id="donate_guide" class="donate_bar center hidden"><img src="http://ww2.sinaimg.cn/small/bcb4c13agw1f7bvfnptyvj20u00xcgp0.jpg" title="微信打赏"></div><script type="text/javascript">document.getElementById('btn_donate').onclick = function(){
    $('#donate_board').addClass('hidden');
    $('#donate_guide').removeClass('hidden');
}</script></div><div data-thread-key="2018/03/17/Knowledge in NLP - Review of Literature/" data-title="Knowledge in NLP - Review of Literature" data-url="http://yoursite.com/2018/03/17/Knowledge in NLP - Review of Literature/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Share/">Paper Share</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technology/">Technology</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/2016/" style="font-size: 15px;">2016</a> <a href="/tags/arXiv/" style="font-size: 15px;">arXiv</a> <a href="/tags/Long/" style="font-size: 15px;">Long</a> <a href="/tags/ACL/" style="font-size: 15px;">ACL</a> <a href="/tags/Paper-Share-20180312-ACL-2017-Carnegie-Mellon-University/" style="font-size: 15px;">(Paper Share-20180312)[ACL 2017| Carnegie Mellon University]</a> <a href="/tags/Data-and-Task-Share-20180305-QA-DBQA/" style="font-size: 15px;">(Data and Task Share-20180305)[QA|DBQA]</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/Relation-Extraction/" style="font-size: 15px;">Relation Extraction</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/03/17/Knowledge in NLP - Review of Literature/">Knowledge in NLP - Review of Literature</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/12/WikiReading - Review of Literature/">WikiReading - Review of Literature</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Attention-over-Attention Neural Networks for Reading Comprehension/">Attention-over-Attention Neural Networks for Reading Comprehension</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Debug心得/">Debug心得</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Gated-Attention Readers for Text Comprehension/">Gated-Attention Readers for Text Comprehension</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Improved Representation Learning for Question Answer Matching/">Improved Representation Learning for Question Answer Matching</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/BUI学习资料/">BUI学习资料</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Webx知道tips/">webx知道</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/Combining Natural Logic and Shallow Reasoning for Question Answering/">Combining Natural Logic and Shallow Reasoning for Question Answering</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/17/ibatis/">ibatis</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.qq.com/404/" title="腾讯公益404" target="_blank">腾讯公益404</a><ul></ul><a href="http://waveli123.github.io/" title="我的博客" target="_blank">我的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">大道至简--博客记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'true'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?true";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>